{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ce0077",
   "metadata": {},
   "source": [
    "### Remix your own scraper\n",
    "\n",
    "This notebook combines several code snippets for scrapers I tend to use again and again. Feel free to copy, paste, delete and comment out any code that is relevant to your personal analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —————— libraries that need to be installed, which you can do via pip ———————\n",
    "\n",
    "import pdfplumber # to scrape pdfs, documentation: https://github.com/jsvine/pdfplumber\n",
    "import requests # to open up live links, documentation: https://docs.python-requests.org/en/latest/\n",
    "from bs4 import BeautifulSoup # to parse HTML, documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "import spacy # to do natural language processing like word counts, documentation\n",
    "\n",
    "\n",
    "# —————— libraries built into Python ———————\n",
    "\n",
    "import json # to read json formatted data\n",
    "import csv # to write and read csv\n",
    "import time # to build in wait time for loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32862cb7",
   "metadata": {},
   "source": [
    "### Using `requests` to open up live URL\n",
    "- use `requests.get()` to get website response for an API feed and a live link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a82b78",
   "metadata": {},
   "source": [
    "#### For an API feed\n",
    "- For APIs: Build your API URL and store it in a variable\n",
    "    - a long string with your API using concatenation\n",
    "    - insert api key into quotation marks or store separately as a file that is not committed via github, open this way: open('../data/api-key.txt').read().strip() \n",
    "    - build base URL according to documentation\n",
    "    - store it as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\" # insert api key into quotation marks or store separately as a file\n",
    "api_url = 'https://www.googleapis.com/youtube/v3/search?key='+api_key+'&part=snippet&channelId=UCJFp8uSYCjXOMnkUyb3CQ3Q' #sample API key is youtube\n",
    "print(api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = requests.get(api_url).text\n",
    "items = json.loads(api_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03586dc",
   "metadata": {},
   "source": [
    "#### For a live link\n",
    "- use `requests.get()` to open URL \n",
    "- use `.content` to get its HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Category:Women_computer_scientists\"\n",
    "page = requests.get(url)\n",
    "page_content = page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81dea2d",
   "metadata": {},
   "source": [
    "#### For polite scrapers (optional)\n",
    "- add information about yourself so server maintenance can contact you\n",
    "- add time between each scrape for any loops so you don't overload the servers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ab2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your identification\n",
    "headers = {\"user-agent\" : \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36(KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36;\",\n",
    "\"from\": \"Your name example@domain.com\"}\n",
    "page_urls = [\"url1.com\", \"url2.com\"]\n",
    "\n",
    "for url in page_urls:\n",
    "    # adds time between pinging the server as to not overload it\n",
    "    time.sleep(2) \n",
    "    # add headers\n",
    "    page = requests.get(url, headers= headers) # `headers= headers` allows websites to trace back who scraped\n",
    "    page_content = page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a008e",
   "metadata": {},
   "source": [
    "### Using `pdfplumber` to extract information from pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(\"path/to/file.pdf\") as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    print(first_page.chars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7504f48",
   "metadata": {},
   "source": [
    "### Using `BeautifulSoup` to parse and `html` file\n",
    "\n",
    "- open the file\n",
    "- parse it through beautiful soup\n",
    "- find the container with your content \n",
    "- find each item and cycle through it\n",
    "- store the data in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba02c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_page.html\") as page:\n",
    "    soup = BeautifulSoup(page,  \"html.parser\")\n",
    "    contents = soup.find(\"div\", class_=\"name_of_class\")\n",
    "    list = contents.find_all( \"div\" , class_=\"name_of_class2\")\n",
    "\n",
    "    for item in list:\n",
    "        content_item = item.find(\"div\", class_=\"name_of_class3\").get_text()\n",
    "        timeaccessed = item.find(\"div\", class_=\"name_of_class4\").get_text()\n",
    "\n",
    "        row = { \"content_item\": content_item,\n",
    "                \"timeaccessed\": timeaccessed\n",
    "              }\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f58e8",
   "metadata": {},
   "source": [
    "### Using `csv` to write scraped information to file\n",
    "- open and create file with a `with open()` statement\n",
    "- created fieldnames if you've stored data as dictionary \n",
    "- write header\n",
    "- write rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5519d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new csv into which we will write all the rows\n",
    "with open(\"../output/csv_name.csv\", \"w+\") as csvfile:\n",
    "    # these are the header names (must correspond with names you gave it while scraping, see above):\n",
    "    fieldnames = [\"content_item\", \"timeaccessed\"]\n",
    "    # this creates your csv\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    # this writes in the first row, which are the headers\n",
    "    writer.writeheader()\n",
    "\n",
    "    # this loops through your rows (the array you set at the beginning and have updated throughtout)\n",
    "    for row in rows:\n",
    "        # this takes each row and writes it into your csv\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c414d",
   "metadata": {},
   "source": [
    "## Word analysis (bonus!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ec00b",
   "metadata": {},
   "source": [
    "### Using `spacy` to do a simple word count\n",
    "\n",
    "Spacy is a library that can assist you in doing linguistic analyses. \n",
    "\n",
    "To install and use the Englis-language version of spacy you should run these commands in your virtual environment:\n",
    "`pip3 install spacy`\n",
    "`python3 -m spacy download en_core_web_sm`\n",
    "\n",
    "In this example, you will be importing the `text.txt` file in our `data` folder.\n",
    "\n",
    "- open document with text\n",
    "- turn it into a `spacy` document/corpus\n",
    "- process tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79552da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# opens the text file and turns it into a string\n",
    "text = open(\"../data/text.txt\",\"r+\").read()\n",
    "len(text) # this returns the length of characters and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "len(doc) # this returns the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for token in doc:\n",
    "    rows.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997f53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d11442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
