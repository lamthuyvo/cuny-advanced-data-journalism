{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia scraper\n",
    "\n",
    "\n",
    "First: Import our modules or packages that we will need to scrape a website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open the web site with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Category:Women_computer_scientists\"\n",
    "page = requests.get(url)\n",
    "page_content = page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse the page through the BeautifulSoup library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "all_groupings = soup.find('div', class_='mw-category')\n",
    "print(all_groupings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_groupings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouping in all_groupings:\n",
    "    print(grouping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isolate all groupings by letter and add them to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouping in all_groupings:\n",
    "    names_list = grouping.find('ul')\n",
    "    category = grouping.find('h3').get_text()\n",
    "    alphabetical_names = names_list.find_all('li')\n",
    "    print(grouping)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop through the groupings and store them as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty array for your data\n",
    "rows = []\n",
    "# loop through each grouping\n",
    "for grouping in all_groupings:\n",
    "    names_list = grouping.find('ul')\n",
    "    category = grouping.find('h3').get_text()\n",
    "    alphabetical_names = names_list.find_all('li')\n",
    "    for item in alphabetical_names:\n",
    "        # get the name\n",
    "        name  = item.text\n",
    "        # get the link\n",
    "        anchortag = item.find('a',href=True)\n",
    "        link = \"https://en.wikipedia.org\" + anchortag['href']\n",
    "        # get the letter\n",
    "        letter_name = category\n",
    "        # make a data dictionary that will be written into the csv\n",
    "        row = { 'name': name,\n",
    "                'link': link,\n",
    "                'letter_name': letter_name}\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print each row into a spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new csv into which we will write all the rows\n",
    "with open('../output/all-women-computer-scientists.csv', 'w+') as csvfile:\n",
    "    # these are the header names:\n",
    "    fieldnames = ['name', 'link', 'letter_name']\n",
    "    # this creates your csv\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    # this writes in the first row, which are the headers\n",
    "    writer.writeheader()\n",
    "\n",
    "    # this loops through your rows (the array you set at the beginning and have updated throughtout)\n",
    "    for row in rows:\n",
    "        # this takes each row and writes it into your csv\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative scraper â€” Making parts of the scripts reuseable, more responsible\n",
    "\n",
    "- announcing who you are using `requests` headers (polite scraping!)\n",
    "- make variables that hold headers and links\n",
    "- turning repetitive steps into functions\n",
    "    - making your scraper take some time (mindful scraping without overloading the server!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your identification\n",
    "headers = {\"user-agent\" : \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36(KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36;\",\n",
    "\"from\": \"Your name example@domain.com\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty array for your data\n",
    "rows = []\n",
    "\n",
    "# open the web site\n",
    "urls = [\"https://en.wikipedia.org/wiki/Category:Women_computer_scientists\", \"https://en.wikipedia.org/w/index.php?title=Category:Women_computer_scientists&pagefrom=Lin%2C+Ming+C.%0AMing+C.+Lin#mw-pages\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(url):\n",
    "    time.sleep(2)\n",
    "    # add headers\n",
    "    page = requests.get(url, headers= headers)\n",
    "    page_content = page.content\n",
    "    # parse the page through the BeautifulSoup library\n",
    "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "    content = soup.find(\"div\", class_=\"mw-category\")\n",
    "    all_groupings = content.find_all(\"div\", class_=\"mw-category-group\")\n",
    "    for grouping in all_groupings:\n",
    "        names_list = grouping.find(\"ul\")\n",
    "        category = grouping.find(\"h3\").get_text()\n",
    "        alphabetical_names = names_list.find_all(\"li\")\n",
    "        for item in alphabetical_names:\n",
    "            # get the name\n",
    "            name  = item.text\n",
    "            # get the link\n",
    "            anchortag = item.find(\"a\",href=True)\n",
    "            link = anchortag[\"href\"]\n",
    "            # get the letter\n",
    "            letter_name = category\n",
    "            # make a data dictionary that will be written into the csv\n",
    "            row = { \"name\": name,\n",
    "                    \"link\": link,\n",
    "                    \"letter_name\": letter_name}\n",
    "            rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    scrape_content(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new csv into which we will write all the rows\n",
    "with open('../output/all-women-computer-scientists.csv', 'w+') as csvfile:\n",
    "    # these are the header names:\n",
    "    fieldnames = ['name', 'link', 'letter_name']\n",
    "    # this creates your csv\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    # this writes in the first row, which are the headers\n",
    "    writer.writeheader()\n",
    "\n",
    "    # this loops through your rows (the array you set at the beginning and have updated throughtout)\n",
    "    for row in rows:\n",
    "        # this takes each row and writes it into your csv\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
