{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae37f5d1-e38d-409f-9ab6-b54556d79eb9",
   "metadata": {},
   "source": [
    "### Text comprehension through spaCy\n",
    "\n",
    "This notebook is an example of topic modeling adapted from [this writeup](https://medium.com/@sayahfares19/text-analysis-topic-modelling-with-spacy-gensim-4cd92ef06e06).\n",
    "\n",
    "It showcases some of the linguistic analysis features that spaCy offers. \n",
    "\n",
    "The libraries we will use are:\n",
    "- `pandas`: for reading in and exporting spreadsheets\n",
    "- `spacy`: a natural language processing library that contains various models trained on various languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced40d95-a0fd-45bc-8e3d-018afc307fe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "# for comprehension of language\n",
    "import spacy \n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47309a2e-25da-4775-8139-1ef3b214fe5a",
   "metadata": {},
   "source": [
    "#### Let's load spaCy's English language trained pipeline\n",
    "\n",
    "`A training pipeline typically reads training data from a feature store, performs model-dependent transformations, trains the model, and evaluates the model before the model is saved to a model registry.`\n",
    "\n",
    "You will need to download one of spaCy's models and can do so by typing this into a cell here:\n",
    "```\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535f8d12-1144-467a-bbaa-ef2700fe5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09006585-4b45-45c9-9578-02a9abe0d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920de05e-8a0e-4ee1-943a-e40900d5a589",
   "metadata": {},
   "source": [
    "Now we can run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b0d3b6-cebb-4c38-85f6-685db54eca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nlp(\"I'm a student who is about to finish graduate school at City University of New York in Manhattan, New York.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ede22-827e-485f-86ae-9dfe805d8351",
   "metadata": {},
   "source": [
    "## Computational Linguistics\n",
    "\n",
    "#### POS-Tagging — (Part Of Speech)\n",
    "spaCy has a nifty way to look into how each word is used in a sentence, often also referred to as Part Of Speech (POS). There are eight main parts of speech — nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions, and interjections. \n",
    "\n",
    "Once we have turned our sentence into an NLP object we can look at the token (meaning the individual words) it consists of and two types of tags to represent what part of speech each word is on a higher and on a more granular level. \n",
    "\n",
    "You can find various tags here and their explanations [here](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/13-POS-Keywords.html#spacy-part-of-speech-tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d730166-04c3-41ca-b14d-28f2151bd37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word I represents this part of speech: PRON and PRP\n",
      "The word 'm represents this part of speech: AUX and VBP\n",
      "The word a represents this part of speech: DET and DT\n",
      "The word student represents this part of speech: NOUN and NN\n",
      "The word who represents this part of speech: PRON and WP\n",
      "The word is represents this part of speech: AUX and VBZ\n",
      "The word about represents this part of speech: ADJ and JJ\n",
      "The word to represents this part of speech: PART and TO\n",
      "The word finish represents this part of speech: VERB and VB\n",
      "The word graduate represents this part of speech: ADJ and JJ\n",
      "The word school represents this part of speech: NOUN and NN\n",
      "The word at represents this part of speech: ADP and IN\n",
      "The word City represents this part of speech: PROPN and NNP\n",
      "The word University represents this part of speech: PROPN and NNP\n",
      "The word of represents this part of speech: ADP and IN\n",
      "The word New represents this part of speech: PROPN and NNP\n",
      "The word York represents this part of speech: PROPN and NNP\n",
      "The word in represents this part of speech: ADP and IN\n",
      "The word Manhattan represents this part of speech: PROPN and NNP\n",
      "The word , represents this part of speech: PUNCT and ,\n",
      "The word New represents this part of speech: PROPN and NNP\n",
      "The word York represents this part of speech: PROPN and NNP\n",
      "The word . represents this part of speech: PUNCT and .\n"
     ]
    }
   ],
   "source": [
    "for token in sent:\n",
    "    print(f\"The word {token.text} represents this part of speech: {token.pos_} and {token.tag_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f4244-3e7f-421a-bcd9-1b6cb96cad2c",
   "metadata": {},
   "source": [
    "#### NER-Tagging — (Named Entity Recognition)\n",
    "Named-entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "SpaCy recognizes the following built-in entity types:\n",
    "- PERSON - People, including fictional.\n",
    "- NORP - Nationalities or religious or political groups.\n",
    "- FAC - Buildings, airports, highways, bridges, etc.\n",
    "- ORG - Companies, agencies, institutions, etc.\n",
    "- GPE - Countries, cities, states.\n",
    "- LOC - Non-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT - Objects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT - Named hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART - Titles of books, songs, etc.\n",
    "- LAW - Named documents made into laws.\n",
    "- LANGUAGE - Any named language.\n",
    "- DATE - Absolute or relative dates or periods.\n",
    "- TIME - Times smaller than a day.\n",
    "- PERCENT - Percentage, including \"%\".\n",
    "- MONEY - Monetary values, including unit.\n",
    "- QUANTITY - Measurements, as of weight or distance.\n",
    "- ORDINAL - \"first\", \"second\", etc.\n",
    "- CARDINAL - Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec16ce0b-8362-4aed-a1d5-737898f38c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      "'m \n",
      "a \n",
      "student \n",
      "who \n",
      "is \n",
      "about \n",
      "to \n",
      "finish \n",
      "graduate \n",
      "school \n",
      "at \n",
      "City ORG\n",
      "University ORG\n",
      "of ORG\n",
      "New ORG\n",
      "York ORG\n",
      "in \n",
      "Manhattan GPE\n",
      ", \n",
      "New GPE\n",
      "York GPE\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "for token in sent:\n",
    "    print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7de4f5-727d-4313-9745-49ed04cbadc1",
   "metadata": {},
   "source": [
    "Let's run a slightly different version of this code to see what role these things play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23721041-a1b2-4c3f-b0e8-98b7d32a3cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City University of New York ORG\n",
      "Manhattan GPE\n",
      "New York GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in sent.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ebccf-1168-4c74-9f42-aef062dbfa86",
   "metadata": {},
   "source": [
    "You can render these roles visually, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff04120-57a7-4902-b43c-3d1db84ed3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I'm a student who is about to finish graduate school at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    City University of New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manhattan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(sent, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f44ebb-9272-4ac6-8173-a32654f6a408",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "spaCy also has some sentiment analysis tools baked into its library. For some rudimentary sentiment analysis we will be using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cbeef51-1283-4d97-b23e-1540e9c18b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacytextblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a686c795-8120-4f91-806d-3dca7ad02971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555882c-7a7c-4de9-8492-41f1e6129b93",
   "metadata": {},
   "source": [
    "As before let's load our model. We'll also load [`spacytextblob`](https://spacy.io/universe/project/spacy-textblob) which as an adaptation of a popular library for sentiment analysis called [`textblob`](https://textblob.readthedocs.io/en/latest/quickstart.html). And funnily enough, textblob's sentiment analysis is based on yet another library called pattern.\n",
    "\n",
    "According to [investigate.ai](https://investigate.ai/investigating-sentiment-analysis/comparing-sentiment-analysis-tools/#TextBlob), \"The sentiment analysis lexicon bundled in Pattern focuses on adjectives. It contains adjectives that occur frequently in customer reviews, hand-tagged with values for polarity and subjectivity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f088c00-fbde-45b5-a621-4a331db314a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7f92724f4fa0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb62607-bb83-4016-a4a2-3b96f8649a83",
   "metadata": {},
   "source": [
    "Then we can get started. Here's a sample text that we will turn into a spaCy document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adb61f6-52b0-4f4e-b12f-9ca9e469fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b5dba-6c6e-442b-808a-ecd3ab70c38c",
   "metadata": {},
   "source": [
    "Once we have done this we can start analyzing it by:\n",
    "- polarity\n",
    "- subjectivity\n",
    "- overall sentiment assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41c947a6-8ffa-4415-8d9f-67828303e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-0.125, subjectivity=0.9)\n"
     ]
    }
   ],
   "source": [
    "print(doc._.blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d783c4b4-e6f6-4b39-b328-7b2e8cb0073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.125\n"
     ]
    }
   ],
   "source": [
    "print(doc._.blob.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d773aa-f919-4a69-94f7-85fb19c54116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(doc._.blob.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1210ab1b-ee4c-4329-87b0-876d7388cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n"
     ]
    }
   ],
   "source": [
    "print(doc._.blob.sentiment_assessments.assessments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad20730-da4a-47f8-8b44-e7697adfe298",
   "metadata": {},
   "source": [
    "### Why do we care? How to apply this to journalism\n",
    "\n",
    "These tools can help us analyze documents in large quantities. Let's apply this to the article about [Bruno, the thicc cat](https://www.buzzfeednews.com/article/juliareinstein/this-thicc-lazy-high-maintenance-incredibly-well-hydrated). \n",
    "\n",
    "Lets say our goal is to run an analysis of every word in our analysis and we want to know the following for each word:\n",
    "- what its name entity is\n",
    "- what its sentiment is\n",
    "\n",
    "For this exercise you will need to:\n",
    "- Create a word list (hint: look at last week!) ← you’ll do this alone\n",
    "- Create a new column for the name entity:\n",
    "  - Use the `.apply()` function to apply a custom function to the column containing the individual words\n",
    "- Create a column for the polarity score\n",
    "    - Use the `.apply()` function to apply a custom function to the column containing the individual words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc96cb-e997-4326-a2dd-eda839a2fdf5",
   "metadata": {},
   "source": [
    "##### Open the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d525c34-822b-43b7-89cd-b5df98397f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2990"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "# opens the text file and turns it into a string\n",
    "text = open(\"../data/text.txt\",\"r+\").read()\n",
    "len(text) # this returns the length of characters and spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516af791-5cde-47b8-a9ed-9faa7a90c5a0",
   "metadata": {},
   "source": [
    "##### Turn text into nlp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48ecc48b-e8c3-4486-8a9c-51c68aeb65f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "len(doc) # this returns the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca130ea4-c43d-4817-94d1-decd8da98b9f",
   "metadata": {},
   "source": [
    "##### Get sentinment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aae04995-1fdf-4689-9f67-51c28b6e57d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.10191029694706165, subjectivity=0.43493619670090267)\n"
     ]
    }
   ],
   "source": [
    "print(doc._.blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cea090-46c4-432b-a27a-04b5090cfc5f",
   "metadata": {},
   "source": [
    "###### Turn article into array of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db07b2e8-87e5-4106-8314-d3ea74fab498",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for token in doc:\n",
    "    rows.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b075e99e-1fa9-46b9-8a69-1bc6d561a945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bruno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word\n",
       "0   This\n",
       "1     is\n",
       "2  Bruno\n",
       "3      ,\n",
       "4    and"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dataframe = pd.DataFrame(rows)\n",
    "word_dataframe.columns = ['word']\n",
    "word_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a13f08-4d47-46d8-b3a3-165d83675e8c",
   "metadata": {},
   "source": [
    "##### Create a new column called \"ner\" and find the name entity for each word\n",
    "\n",
    "Make sure you apply a function to this column. \n",
    "\n",
    "Here's the syntax of a function: \n",
    "```\n",
    "def addnumbers(x):\n",
    "\tnew_number = 2 + x\n",
    "\treturn new_number\n",
    "```\n",
    "Here's how you apply it:\n",
    "```\n",
    "df[\"column\"].apply(addnumbers)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75041776-974c-4741-a072-7cc5cc1da946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getner(x):\n",
    "    doc = nlp(x)\n",
    "    for ent in doc.ents:\n",
    "        return ent.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7244b2d5-8c84-4f6f-8fcd-141f245bd9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataframe[\"ner\"] = word_dataframe[\"word\"].apply(getner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86033c5-1b13-4ecb-a074-6e7074bc5520",
   "metadata": {},
   "source": [
    "##### Summarize the dataframe of words via the \"ner\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b99ecfb6-cbd5-4426-9cea-f4d0b9876867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ner\n",
       "PERSON      8\n",
       "CARDINAL    3\n",
       "GPE         3\n",
       "ORG         3\n",
       "DATE        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dataframe[\"ner\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08cd833d-8874-4176-84d1-1bd40ab10bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataframe[\"ner\"].value_counts().to_csv(\"../output/entities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e8e64-ac21-4f2b-91ea-5a5653e09711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4772104-a23f-4956-a13e-ca0bd4345a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
