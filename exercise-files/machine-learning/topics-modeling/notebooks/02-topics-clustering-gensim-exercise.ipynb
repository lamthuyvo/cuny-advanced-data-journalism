{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae37f5d1-e38d-409f-9ab6-b54556d79eb9",
   "metadata": {},
   "source": [
    "### Text cleaning and topic modeling\n",
    "\n",
    "This notebook is an example of topic modeling adapted from [this writeup](https://medium.com/@sayahfares19/text-analysis-topic-modelling-with-spacy-gensim-4cd92ef06e06).\n",
    "\n",
    "It performs the following tasks:\n",
    "\n",
    "- the first part of the notebook loads texts from a spreadsheet and turns them into one large corpuse\n",
    "- then we walk through various ways in which we can analyze and clean our corpus using spaCy (this includes taking out `stopwords` — words most often used in the English language and lemmatizing our corpus)\n",
    "- to better understand how a model works this notebook also explores some funcationalities of spaCy\n",
    "- the last parts of this notebook then make a simple topics model from the cleaned language data\n",
    "\n",
    "The libraries we will use are:\n",
    "- `pandas`: for reading in and exporting spreadsheets\n",
    "- `spacy`: a natural language processing library that contains various models trained on various languages\n",
    "- `gensim`: a library for topic modelling, document indexing and similarity retrieval with large corpora. In this case we will use it for topic modeling, the process of clustering words that seem to be used a lot in relation to one another. The algorithms built into genim that this notebook uses are called [Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2) and [Latent Semantic Analysis (LSA)](https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/).\n",
    "- `pyLDAvis`: a library that is capable of visualizing your topics clusters.\n",
    "\n",
    "Topic modeling is a form of unsupervised machine learning and can be really helpful in discovering topics in a large amount of text, especially if you're uncertain which topics might be buried in thousands or millions of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced40d95-a0fd-45bc-8e3d-018afc307fe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for comprehension of language\n",
    "\n",
    "# for topics modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47309a2e-25da-4775-8139-1ef3b214fe5a",
   "metadata": {},
   "source": [
    "### Load spaCy's English language trained pipeline\n",
    "\n",
    "`A training pipeline typically reads training data from a feature store, performs model-dependent transformations, trains the model, and evaluates the model before the model is saved to a model registry.`\n",
    "\n",
    "You will need to download one of spaCy's models and can do so by typing this into a cell here:\n",
    "```\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f8d12-1144-467a-bbaa-ef2700fe5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09006585-4b45-45c9-9578-02a9abe0d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the English language model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18926fb2-aa30-4e55-8dcc-0e41fbed4a71",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    "\n",
    "A lot of languages also contain 'stop words', words that are used very frequently and may not be useful when we're evaluating how often certain words may be used. spaCy has niftyfunctions that allow us to designate stop words for our analysis. \n",
    "\n",
    "For this purpose, we got stopwords [here](https://gist.github.com/sebleier/554280).\n",
    "\n",
    "First we need to open the text file adn then turn it into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9e53a-24fa-418f-b368-d494b6aa7e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9343e2a1-7878-445a-b7e6-5a1addb27aa2",
   "metadata": {},
   "source": [
    "Next we use spaCy's model and define stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113aa82-5a8a-40cc-9b80-a90236dc48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts a loop that iterates through each word in the stop_words list.\n",
    "\n",
    "    # This line retrieves the lexeme (the base or dictionary form of a word) from spaCy's vocabulary. \n",
    "\n",
    "    # then we set `lexeme.is_stop = True`for each word, making every word a stop word in spaCy's vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbdecb-b221-4888-b06a-0ae2b2f9f4ed",
   "metadata": {},
   "source": [
    "## Loading your text and making it a corpus\n",
    "\n",
    "#### First we need to load the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9bbc4d-422e-41ce-b170-1581aecd2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a spreadsheet with the text you want to analyze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509699aa-b11b-425a-b9b7-846f0372ba59",
   "metadata": {},
   "source": [
    "The next lines take all content from the `transcript` column, turn it into a list and then join it all with a space between each text. This creates one large corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8065628-6859-48ac-97c7-857d9840b038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e829d05-4fe3-4504-82b0-fb5c4eefc9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b3040-370c-4d0f-94b9-44665c4c612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf634d9-48de-4743-9592-fa53a06d0710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ff990b1-1877-49ca-b463-7d63aa5c8f2c",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "The next few lines 'normalize' the text and turns words into lemmas, get rid of stopwords and punctuation markers, and add lemmatized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcefef3-b7a5-4ee3-ab29-79b6cb3dcd07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here's a demo of us cycling through the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b42250-57a4-458b-9de2-21dbaba84e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbb2dd-5088-4da9-8cdf-5ebaa3c1a100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We add some words to the stop word list\n",
    "\n",
    "#let's create some empty arrays. \n",
    "# texts will hold all our words that we will use for our topic model\n",
    "\n",
    "# is a temporary array that we will use to store the lemma-version of a word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeffef5-79c4-4499-840e-347c46c66ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9ffba45-d6fa-44e3-915c-deb07ac9d0fb",
   "metadata": {},
   "source": [
    "In the next lines we turn these cleaned texts into a bag-of-words format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe4a03a-cabc-4ea1-b663-7a96dfbb53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary() to map each unique word to a unique integer ID\n",
    "\n",
    "\n",
    "# this line creates a corpus and converts a single document (a list of words) into a bag-of-words format\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce230e8-c30b-43d8-93ef-8f68e9cda7ab",
   "metadata": {},
   "source": [
    "## Different Kinds of Topic Modeling \n",
    "Topic Modeling refers to the probabilistic modeling of text documents as topics. Gensim is one of the most popular libraries to perform such modeling.\n",
    "\n",
    "#### LSI — Latent Semantic Indexing\n",
    "One of the methods available in gensim is called LSI, which stands for Latent Semantic Indexing. LSI aims to find hidden (latent) relationships between words and concepts in a collection of documents. The assumption here is words that are used in similar contexts tend to have similar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbbb000-7349-4ed7-8403-3e35ac591b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "231fe9a6-42b5-45cb-91c9-14f699317b78",
   "metadata": {},
   "source": [
    "#### HDP — Hierarchical Dirichlet Process\n",
    "HDP, the Hierarchical Dirichlet Process is an unsupervised Topic Model which figures out the number of topics on its own. HPD assumes that documents are mixtures of topics, and topics are mixtures of words, but it doesn't limit the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea15908-f386-4187-a846-182ab2ec4e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4eff84d-1d3c-4a10-85a5-7b792116f635",
   "metadata": {},
   "source": [
    "#### LDA — Latent Dirichlet Allocation\n",
    "LDA or Latent Dirichlet Allocation is arguably the most famous Topic Modeling algorithm out there. Out here we create a simple Topic Model with 5 topics. The LDA algorithm assumes that each document is a mixture of topics, and each topic is a mixture of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24830a4c-8ed6-498d-affc-81be19401d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a93f3d12-9624-4700-8662-a43d0305dfca",
   "metadata": {},
   "source": [
    "## Visualizing Topics with pyLDAvis\n",
    "pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n",
    "\n",
    "The visualization is intended to be used within an IPython notebook but can also be saved to a stand-alone HTML file for easy sharing.\n",
    "\n",
    "**Note: If you have issues running the `pyLDAvis.gensim_modeles.prepare()` function, you may need to walk yourself through [these fixes](https://docs.google.com/document/d/1XOz5fJdHR754SHkMIrqCxkgD2yGHhBlueK4EcbaNwII/edit?usp=sharing).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359801b-6e47-4fa3-9026-a1a82b590233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for visualizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc7dba-559b-4ced-9889-204b5df135da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9a870-2e09-45cb-8af5-cd4029c36d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e38eb-c485-4434-83e3-1102152c24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5fc1b-6e06-4ffc-b832-e1f045aedc18",
   "metadata": {},
   "source": [
    "## Word count\n",
    "For good measure, we can also use this space to make a word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a2a3b-742c-40af-85ff-fe4f0133a336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19873bb5-106f-4347-9f3b-8e79c4fde9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d82c3-8481-4e42-a0c6-4d1ac113d09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f02bd-e91c-4ac8-ac0f-b2be718adca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f088c00-fbde-45b5-a621-4a331db314a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
